---
# Blog post title
title: "Dockers with PMDK"

# Blog post creation date
date: 2022-11-08

# Change to 'false' when publishing the blog post
draft: false

# Blog post description
description: "Dockers usage in PMDK"

# Blog post hero image. Used to override the default hero background image.
# eg: image: "/images/my_blog_heroimg.png"
hero_image: ""

# Blog post thumbnail
# eg: image: "/images/posts/my_blog_thumbnail.png"
image: ""

# Blog post author
author: "lukaszstolarczuk"

# Categories to which this blog post belongs
blogs: ['PMDK']
# Blog tags
tags: ["Intro", "dockers", "pmdk"] # XXX add memkind perhaps

# Blog post type
type: "post"
---

In this blog post, I'll describe why we believe dockers are easy to use, time-saving, and very
useful. If you have never heard of dockers (or containers in general), please read, for example,
[this overview](https://docs.docker.com/get-started/overview/). We use dockers in almost
all of our repositories in pmem organization. In this blog post, I will describe
how we use dockers based on the [pmemkv](https://github.com/pmem/pmemkv) repository.
In some of our repositories, like in [PMDK](https://github.com/pmem/pmdk), we use a few
more scripts with more use cases to cover, so it gets a little more complicated.
The section ["Our various solutions"](#our-various-solutions) below describes some
differences between our repositories. Let's get to the details!

<!--
TODO:
* replace pmemkv with PMDK
 ** move some of "differences" to the main content
 ** update links to repos
* add memkind and possible other references
* update the date
-->

# Dockers are easy and maintainable

From our point of view, dockers significantly simplify of part of our work.
We use dockers in our CI for the majority of tests' executions. We've started by preparing docker
images for commonly used Linux distributions. Each of our repositories contain `utils/docker`
directory, like [here](https://github.com/pmem/pmemkv/tree/master/utils/docker). Images are stored
in a separate sub-directory, unsurprisingly called `images`. Among docker 'recipes' (Dockerfiles),
there lay down some scripts used within containers to install dependencies (e.g., `install-rapidjson.sh`)
and a short README file with instructions on manually building and running dockers from our Dockerfile
images. Note that we only have docker images for publicly open Linux distributions; some of our
repositories/libraries support Windows' builds, and its testing is done purely on Virtual Machines
delivered by CI providers (like GitHub Actions).

# Cleaner workflow and reproducible environment

We figured that keeping Linux images as files simplifies their maintenance and updates, makes them
publicly available in the repo (for any reviews and contributions) and allows for easy re-build on
demand by anyone. The last part proved to be especially useful for us - developers. Execution of tests
is just the beginning. If the CI fails, you have to dig through the CI's logs and try to guess what
happened. With a quickly reproducible environment, the job got a lot easier. We don't have to look
for a specific machine with some precise version of a Linux distribution. We read our README ;-)
and simply build docker image on any developer's machine and debug the code within the exactly
same environment.

To be even lazier... I mean, productive... we sped up the whole process. GitHub comes with a great
feature - ["Packages"](https://github.com/features/packages). It allows us to store built docker
images for later re-use. We use it mainly on CI to save time on re-building (an unchanged image) and
get to tests' execution as soon as possible. If the image(s) or installation scripts are updated,
we have to re-built them, but only once. Such ready-to-use docker images can be easily stored as
a "package(s)" along the GitHub project - see e.g.,
[pmemkv's package](https://github.com/pmem/pmemkv/pkgs/container/pmemkv). Each image is tagged
by distribution's name, its purpose (e.g., image `ubuntu-20.04_bindings` for testing
pmemkv's bindings), and a release version (because pmemkv's dependencies may have changed
over time, in various release branches).

If you want to download and run one of our published images, it's as simple as
entering any of our, mentioned above, public "packages", like
[pmemkv/fedora-rawhide-latest](https://github.com/pmem/pmemkv/pkgs/container/pmemkv/39805418?tag=fedora-rawhide-latest).
There's a straightforward "how to" page generated by GitHub, about using such an image. You can
either `pull` the image and just run it or use it as a base image for your own Dockerfile 'recipe'.
Building on top of our image gives you the advantage of having all dependencies already prepared.
Such an image would have to be expended with potential custom packets and files, e.g., for further
development or debugging.

# Usage in the Continuous Integration

As I mentioned above, we keep all "docker" related files, in most of our repositories,
in a `utils/docker` part of the tree. For example, in pmemkv it is
[here](https://github.com/pmem/pmemkv/tree/master/utils/docker).
The previously mentioned `images` directory contains:
* README file - to explain basics on how to use dockers
* Dockerfiles, which define steps to create our complete OS environments - docker 'recipes'
* installation scripts (for various dependencies and, e.g., testing tools), used to build images
* two helper scripts for building and pushing images to the "registry" (a.k.a. GitHub's Packages)

Dockerfile 'recipes' are used by the helper scripts, and they, in turn, are used by the
"upper" layer of scripts that reside directly in the `utils/docker` directory. `pull-or-rebuild-image.sh`
script makes use of these helpers. It decides whether to re-build the image or just download it
from the registry. Images' re-building is based on changes introduced by a user (e.g., in
a Pull Request).

In 'docker' directory, there are also some `build*` scripts (usually just one - `build.sh`) and several
`run-*` scripts. The first one is used by our CI as an entry point to prepare the environment
(dockers) and execute a selection of our tests. The second group of scripts is prepared for executing
specific sets of checks.

All GitHub Actions workflows and jobs (using the listed above scripts) are defined in
`.github/workflows` sub-tree of our repositories. For pmemkv it is
[here](https://github.com/pmem/pmemkv/tree/master/.github/workflows).

# Our various solutions

As I wrote in the introduction, not all repositories are handled exactly the same. Each repository
in pmem organization was developed by various people with heads full of ideas. Some requirements may
have forced teams to update testing workflows accordingly to their needs. Some changes to CI and
dockers were introduced in a rush (e.g., because of some deadlines) and were not ported to other
repositories. Having said that, we tried to keep the differences to the minimum and level as many
scripts and workflows as possible.

The main differences are, for sure, in `run-*.sh` scripts, which are delivered specifically to
execute tests and checks adequate to the given library. Some repositories, like
[PMDK](https://github.com/pmem/pmdk/tree/master/utils/docker) introduced two `build*` scripts.
Building the whole environment and tests' execution for PMDK requires a significant amount of env
variables to be set on the host machine. To ease the process, there is one script used in CI and
one for an easier local building experience.

PMDK repository is tested in more environments (including various architectures) so some additional
files are located in `utils/docker` to handle different CI's.

Finally, not all repositories share the same number of docker images. Each repository has its own
set of OSes, depending on the requirement for a specific library. We started some efforts
to unify it, but this isn't a piece of cake, and it'd require some time to finish this up.
Currently, some "common" dockers are located in a separate repository called
[dev-utils-kit](https://github.com/pmem/dev-utils-kit).

# Summary

To summarize dockers' usage in PMDK, I'd have to say: it's very nice to have them
working in our CI! As I described above, there are multiple benefits of introducing them
into our development process, with "reproducible" and "portability" as one of the greatest
(in my opinion). Overall, they add a little complexity to our workflows, but after
you get used to these virtual environments - they are great!

As for our various repositories - the testing environments and Continuous Integrations
come in a few flavors, but they are generally quite similar. The differences result from
various needs and different developers, but when you familiarize yourself with any of them,
the rest should be just as readable.
