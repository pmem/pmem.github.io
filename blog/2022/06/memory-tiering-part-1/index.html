<!doctype html><html dir=ltr lang=en-us><head><link rel=stylesheet type=text/css href=/css/style.css><meta property="og:title" content="Memory Tiering (part 1)"><meta property="og:description" content="Extending memory capacity with PMEM Databases such as Redis (an in-memory key-value open-source database) consume a lot of memory. Since fast access is essential for them, they use DRAM to store their data. DRAM is quite expensive and has limited capacity, so a solution we propose in this blog post is to use PMEM (and in the future other types of memory available through CXL - see a pmem.io blog post about it)."><meta property="og:type" content="article"><meta property="og:url" content="https://pmem.io/blog/2022/06/memory-tiering-part-1/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-06-22T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-22T00:00:00+00:00"><meta charset=utf-8><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><title>Memory Tiering (part 1)</title><meta name=author content="PMem.io"><meta name=description content="Persistent Memory Development Kit (PMDK) provides support for transactional and atomic operations to keep the data consistent and durable.  PMDK is a collection of open-source libraries and tools that are available for both Linux and Windows OS.  PMDK facilitates persistent memory programming adoption with higher level language support.  Currently, Java, Python, Rust, Go, C and C++ support is fully validated and delivered on Linux and Windows.  This new generation of persistent memory from Intel has introduced a third memory tier (memory persistence, memory tiering).  In addition to memory and storage tiers, the persistent memory tier offers greater capacity than DRAM and significantly faster performance than storage.  Applications can access persistent memory-resident data structures in-place, like they do with traditional memory, eliminating the need to page blocks of data back and forth between memory and storage. PMDK provides a toolkit for memory hierarchy, memory caching, virtual memory and memory tiering.  PMDK-PMEM toolkit provides operational modes in either app direct mode or memory mode. App Direct Mode provides memory persistent, high availability less downtime and significantly faster storage.  In memory mode provides high memory capacity at lower cost and is transparent to applications.  Memory is volatile in memory mode and persistent in App Direct mode"><meta name=robots content="index, follow, archive"><link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700,900&display=swap" rel=stylesheet type=text/css><link rel=stylesheet href=/css/bootstrap.css type=text/css><link rel=stylesheet href=/css/style.css type=text/css><link rel=stylesheet href=/css/dark.css type=text/css><link rel=stylesheet href=/css/font-icons.css type=text/css><link rel=stylesheet href=/css/animate.css type=text/css><link rel=stylesheet href=/css/magnific-popup.css type=text/css><link rel=stylesheet href=/css/et-line.css type=text/css><link rel=stylesheet href=/css/components/bs-switches.css type=text/css><link rel=stylesheet href=/css/custom.css type=text/css><meta name=viewport content="initial-scale=1,viewport-fit=cover"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css type=text/css><link rel=stylesheet href="/css/colors.php?color=FE9603" type=text/css><link rel=stylesheet href=/css/template/fonts.css type=text/css><link rel=stylesheet href=/css/template/seo.css type=text/css></head><body class=stretched><div id=wrapper class=clearfix><header id=header class="transparent-header floating-header header-size-md sticky-header"><div id=header-wrap class=dark-mode><div class="container dark-mode"><div class=header-row><div id=logo class=logo_dark><a href=/ class=standard-logo data-dark-logo=images/logo-dark.png><img src=https://pmem.io/images/pmem_logo.png alt=PMem.io></a>
<a href=/ class=retina-logo data-dark-logo=images/logo-dark@2x.png><img src=https://pmem.io/images/pmem_logo.png alt=PMem.io></a></div><div id=logo class=logo_light><a href=/ class=standard-logo data-dark-logo=images/logo-dark.png><img src=https://pmem.io/images/pmem_logo_white.png alt=PMem.io></a>
<a href=/ class=retina-logo data-dark-logo=images/logo-dark@2x.png><img src=https://pmem.io/images/pmem_logo_white.png alt=PMem.io></a></div><div class=header-misc><div id=top-search class=header-misc-icon><a href=# id=top-search-trigger><i class=icon-line-search></i><i class=icon-line-cross></i></a></div><div class=top-links><ul class=top-links-container><li><div id=darkSwitch class="dark-mode header-misc-icon d-md-block"><a href=#><i id=darkSwitchToggle></i></a></div></li></ul></div></div><div id=primary-menu-trigger><svg class="svg-trigger" viewBox="0 0 100 100"><path d="m30 33h40c3.722839.0 7.5 3.126468 7.5 8.578427C77.5 47.030386 74.772971 50 70 50H50"/><path d="m30 50h40"/><path d="m70 67H30s-7.5-.802118-7.5-8.365747C22.5 51.070624 30 50 30 50h20"/></svg></div><nav class="primary-menu with-arrows"><ul class=menu-container><li class="menu-item mega-menu"><div class=menu-link><div><a href=/developer-hub>Developer Hub</a></div></div><div class="mega-menu-content mega-menu-style-2 px-0"><div class="container dark-mode"><div class=row><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class=fbox-content><p class=fw-bold>For Developers</p><p>Everything you need to know about Persistent Memory.</p></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/persistent-memory/getting-started-guide><p>Get started <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/pmdk><p>PMDK <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/repoindex><p>PMem Repositories <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/pmemkv><p>PMemKV <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/pmemstream><p>PMemStream <i class=icon-angle-right></i></p></a></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/memkind><p>Memkind <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/miniasync><p>MiniAsync <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/community/#newsletter><p>Newsletter <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://tieredmemdb.github.io/><p>TieredMemDB <i class=icon-angle-right></i></p></a></div></div></div></div></div></div></li><li class="menu-item mega-menu"><div class=menu-link><div><a href=/learn>Learn</a></div></div><div class="mega-menu-content mega-menu-style-2 px-0"><div class="container dark-mode"><div class=row><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class=fbox-content><p class=fw-bold>Access our Documentation</p><p>Learn more about Persistent Memory features and capabilities.</p></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/books><p>Books <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/persistent-memory/><p>Docs <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/glossary><p>Glossary <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/ipmctl-user-guide/><p>ipmctl User Guide <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://docs.pmem.io/ndctl-user-guide/><p>ndctl User Guide <i class=icon-angle-right></i></p></a></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/faq><p>FAQ <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/knowledgebase><p>Knowledge base <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/tutorials><p>Tutorials <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/videos><p>Videos <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/webinars><p>Webinars <i class=icon-angle-right></i></p></a></div></div></div></div></div></div></li><li class="menu-item mega-menu"><div class=menu-link><div><a href=/community>Community</a></div></div><div class="mega-menu-content mega-menu-style-2 px-0"><div class="container dark-mode"><div class=row><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class=fbox-content><p class=fw-bold>Get Connected</p><p>Join an ever-growing community of PMDK-PMEM developers online or in person.</p></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/events><p>Events <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://groups.google.com/group/pmem><p>Forum <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=https://pmem-io.slack.com/join/shared_invite/enQtNzU4MzQ2Mzk3MDQwLWQ1YThmODVmMGFkZWI0YTdhODg4ODVhODdhYjg3NmE4N2ViZGI5NTRmZTBiNDYyOGJjYTIyNmZjYzQxODcwNDg#/shared-invite/email><p>Slack channel <i class=icon-angle-right></i></p></a></div></div></div><div class="mega-menu-column sub-menu-container col-lg-4 border-bottom py-4"><div class=feature-box><div class="fbox-content h-bg-light"><a href=/announcements><p>Announcements <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/blog/2021/10/how-to-contribute-to-pmem.io/><p>Contribute <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/community/#newsletter><p>Newsletter <i class=icon-angle-right></i></p></a></div><div class="fbox-content h-bg-light"><a href=/community/#social-media><p>Social Media <i class=icon-angle-right></i></p></a></div></div></div></div></div></div></li><li class=menu-item><a class=menu-link href=https://pmem.io/solutions><div>Solutions</div></a></li><li class=menu-item><a class=menu-link href=https://pmem.io/blog><div>Blog</div></a></li><li class=menu-item><a class=menu-link href=https://pmem.io/about><div>About</div></a></li></ul></nav><form class=top-search-form method=get><input id=bcs-searchbox aria-label="Search input" type=text name=q class="form-control bcs-searchbox pt-4" placeholder="Type & Hit Enter.." autocomplete=off></form></div></div></div><div class="header-wrap-clone dark-mode"></div></header><div id=customSearch><div id=bcs_js_snippet></div></div><section id=content><div class="content-wrap dark-mode"><div class="container clearfix"><div class="row gutter-40 col-mb-80"><div class="postcontent col-lg-9 order-lg-last"><div class="single-post mb-0"><div class="entry clearfix"><div class=entry-title><h2>Memory Tiering (part 1)</h2></div><div class=entry-meta><ul><li><i class=icon-calendar3></i> 22 Jun, 2022</li><li><i class=icon-user></i> Rafał Rudnicki</li><li><i class=icon-folder-open></i>
PMDK</li></ul></div><div class="entry-content mt-0"><h2 id=extending-memory-capacity-with-pmem>Extending memory capacity with PMEM</h2><p>Databases such as <a href=https://github.com/redis/redis>Redis</a> (an in-memory key-value open-source database) consume a lot of memory.
Since fast access is essential for them, they use DRAM to store their data.
DRAM is quite expensive and has limited capacity, so a solution we propose in this blog post is to use PMEM
(and in the future other types of memory available through CXL - see a <a href=/blog/2022/01/disaggregated-memory-in-pursuit-of-scale-and-efficiency>pmem.io blog post about it</a>).
PMEM can be used in two modes - <a href=/glossary/#memory-mode>Memory Mode</a> and <a href=/glossary/#app-direct>App Direct</a>. In Memory Mode,
user sees DRAM and PMEM memory as single, combined memory node. In App Direct mode,
when user <a href=/blog/2020/01/memkind-support-for-kmem-dax-option>configures PMEM as KMEM DAX</a>, the PMEM memory will be directly available in the system and visible
as a separate NUMA node without a CPU.</p><h2 id=memory-tiering---why-should-i-care>Memory tiering - why should I care?</h2><p>The PMEM in KMEM DAX is available to the application just like regular memory.
When the application allocates data, in the simplest case, the kernel chooses a memory
NUMA node to allocate from in the order determined by the distance between a NUMA node
and the CPU core that made a request (it can be influenced, e.g., by mbind).
The distances matrix between all NUMA nodes can be determined using the <code>numactl -H</code> command:</p><p>Let&rsquo;s consider an example output from a 4-socket machine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>$</span> numactl <span style=color:#f92672>-</span>H
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>available: <span style=color:#ae81ff>4</span> nodes (<span style=color:#ae81ff>0</span><span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>0</span> cpus: <span style=color:#ae81ff>0</span> (...) <span style=color:#ae81ff>95</span>
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>0</span> size: <span style=color:#ae81ff>257579</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>0</span> free: <span style=color:#ae81ff>1535</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>1</span> cpus: <span style=color:#ae81ff>32</span> (...) <span style=color:#ae81ff>127</span>
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>1</span> size: <span style=color:#ae81ff>257985</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>1</span> free: <span style=color:#ae81ff>840</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>2</span> cpus:
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>2</span> size: <span style=color:#ae81ff>126976</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>2</span> free: <span style=color:#ae81ff>126911</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>3</span> cpus:
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>3</span> size: <span style=color:#ae81ff>126976</span> MB
</span></span><span style=display:flex><span>node <span style=color:#ae81ff>3</span> free: <span style=color:#ae81ff>44294</span> MB
</span></span><span style=display:flex><span>node distances:
</span></span><span style=display:flex><span>node   <span style=color:#ae81ff>0</span>   <span style=color:#ae81ff>1</span>   <span style=color:#ae81ff>2</span>   <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  <span style=color:#ae81ff>0</span><span style=color:#f92672>:</span>  <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>20</span>  <span style=color:#ae81ff>17</span>  <span style=color:#ae81ff>28</span>
</span></span><span style=display:flex><span>  <span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>  <span style=color:#ae81ff>20</span>  <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>28</span>  <span style=color:#ae81ff>17</span>
</span></span><span style=display:flex><span>  <span style=color:#ae81ff>2</span><span style=color:#f92672>:</span>  <span style=color:#ae81ff>17</span>  <span style=color:#ae81ff>28</span>  <span style=color:#ae81ff>10</span>  <span style=color:#ae81ff>28</span>
</span></span><span style=display:flex><span>  <span style=color:#ae81ff>3</span><span style=color:#f92672>:</span>  <span style=color:#ae81ff>28</span>  <span style=color:#ae81ff>17</span>  <span style=color:#ae81ff>28</span>  <span style=color:#ae81ff>10</span>
</span></span></code></pre></div><p>Assuming the process is on CPU0 and without specifying the target NUMA node,
the order of used NUMA nodes will be DRAM 0, PMEM 0, DRAM 1, and finally PMEM 1.</p><p><img src=/images/posts/tiering-1-distances.png alt=distances></p><p>So, if the app wants to consume all memory in the system,
it could use all DRAM + PMEM capacity without any code modification or other tricks,
but not having control over the allocation destination has its downsides.
The two most significant are variable latency of data accesses
and unpredictable resource assignment in the multi-app scenario.</p><h3 id=variable-latency-of-data-accesses-problem>Variable latency of data accesses problem</h3><p>Let&rsquo;s suppose our application is a simple database: interface, hash map with keys, and data.
Typically, in such applications, some of the data are more and some are less frequently accessed,
with the distribution being somewhere between <a href=https://en.wikipedia.org/wiki/Normal_distribution>normal</a>, <a href=https://en.wikipedia.org/wiki/Pareto_distribution>Pareto</a>,
or <a href=https://en.wikipedia.org/wiki/Zipf%27s_law>Zipf</a>. On the other hand, some of the data may be placed on the memory tier that
offers low or high access latency. When these two facts are combined,
it may turn out that the kernel has placed the hot data on a low-performance (high-latency) numa node,
and the cold data on high-performance (low-latency) one. Which is the opposite of what we want.</p><p><img src=/images/posts/tiering-1-prob.png alt=prob></p><h3 id=multiapp-problem>Multi–app problem</h3><p>When the system is running many applications or more than one instance of a memory-intensive one,
and they are executed on CPUs belonging to different sockets,
the situation becomes even more complicated. Depending on when the application starts
and how often it allocates, it may turn out that the ratio of e.g. DRAM : PMEM
for one instance is completely different than for another. Instances with lower than
the desired DRAM : PMEM ratio could perform below the required SLA for IOPS or latency requirements.</p><p><img src=/images/posts/tiering-1-ratio.png alt=ratio></p><h2 id=the-solution--memory-tiering>The solution – memory tiering</h2><p>Memory tiering is about the distribution of allocations among all (or selected)
memory tiers. In this series of blog posts,
we will be classifying different types of memory tiering depending on the scale
of changes required to apply to an application code:</p><ul><li>explicit tiering (e.g. using libnuma or Memkind API) - where the application is aware
of the existence of NUMA nodes with different properties and it fully manages the target
NUMA allocation node itself. This is the most powerful and potentially best performing mode,</li><li>explicit auto-tiering (e.g. using Memkind API and memory kinds based on STATIC_RATIO or
DYNAMIC_TIERING policy at the same time) - as above, but the application may delegate
the management of part of the allocations to the tiering library. The application is also
responsible for the proper configuration of the auto-tiering, e.g. defining the number and
type of tiers, the target ratio between tiers, and additional properties of the tiering policy,</li><li>explicit binding (e.g. using numactl tool) - the application is bound to a specific NUMA
nodes in the system and thus allocations made by the application are only applied to these nodes.
Moreover, the numactl tool allows choosing different policies that define what happens
when the application runs out of space on the nodes it is bound to. Using explicit bindings,
one can partially solve the multi – app problem. What is important,
no changes to the application code are needed here,</li><li>transparent tiering (e.g. using autonuma feature in the Linux Kernel,
Memkind Transparent Tiering feature, MemVerge Memory Machine) - the application is not aware
that it is running on a system with multiple NUMA nodes differing in properties.
No change in the application code is required to use such tiering, but the system administrator
must set the tiering properties through configuration files or environment variables.</li></ul><p>We will describe explicit tiering and, partially, explicit auto-tiering in more detail below.</p><h2 id=explicit-tiering>Explicit tiering</h2><h3 id=simple-algorithm-that-keeps-targeting-drampmem-ratio>Simple algorithm that keeps targeting DRAM:PMEM ratio</h3><p>To partially eliminate the problems described above, the user can change the application code
to allocate data from different tiers alternately. Additionally,
the allocator can count how much data has been placed on each tier and
avoid/select those tiers whose occupancy differs from the level
defined by the ratio. This way, the user can both control the ratio of memory
resource usage and balance the latency of data accesses.</p><p><img src=/images/posts/tiering-1-ratio2.png alt=ratio2></p><p><img src=/images/posts/tiering-1-prob2.png alt=prob2></p><h3 id=libnuma-library>Libnuma library</h3><p>For those who would like to write an allocator with an explicit tiering
feature by themselves, the libnuma library can be very helpful.
This library contains various functions for detecting basic properties of NUMA nodes
(capacity, free space, and distance between nodes),
functions to allocate data on specific NUMA nodes, and much more.
This way, the application can identify NUMA nodes with specific properties
and decide which data suits best which NUMA node. What is important,
functions allocating on specific NUMA nodes are relatively slow compared
to their counterparts from the standard glibc library, because they operate on whole pages.
More details about the libnuma library could be found <a href=https://man7.org/linux/man-pages/man3/numa.3.html>here</a>.</p><h3 id=memkind-library>Memkind library</h3><p>The <a href=https://github.com/memkind/memkind>Memkind</a> library allows allocating data directly on different memory tiers
in a convenient way using an API similar to glibc’s malloc,
supplemented with an additional parameter specifying the target &ldquo;kind&rdquo; of memory.
Using this parameter, the user can allocate data directly to DRAM, PMEM
set in KMEM DAX or FSDAX mode, or HBM. Memkind also offers the more intuitive
tiers description like “lowest latency”, “highest capacity” or “highest bandwidth”,
which are set to the appropriate NUMA nodes depending on the corresponding
parameters of existing NUMA nodes (see <a href=/blog/2021/05/memkind-support-for-heterogeneous-memory-attributes/>this</a> blog post for more info).
Notably, Memkind uses a (slightly modified) jemalloc allocator,
which offers good performance and low memory fragmentation.</p><h2 id=explicit-auto-tiering>Explicit auto-tiering</h2><p>We can imagine a situation where a memory-intensive application wants
to always keep some data on the medium offering the fastest access
but has no requirements for the access speed to the remaining data.
The Memkind library is ideally suited for such a scenario. For data requiring
access with the lowest latency, the user simply chooses the “lowest latency” memory kind.
For the remaining data, one could create its type of auto-tiered kind
(using internal API) based on one of the policies offered by Memkind,
such as “static ratio” or “dynamic threshold”. Below is an example of how to do that:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-c data-lang=c><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;memkind.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e>#include</span> <span style=color:#75715e>&lt;memkind/memkind_memtier.h&gt;</span><span style=color:#75715e>
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>struct</span> memtier_builder <span style=color:#f92672>*</span>builder <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    memtier_builder_new(MEMTIER_POLICY_STATIC_RATIO);
</span></span><span style=display:flex><span><span style=color:#75715e>// we want to keep DRAM to PMEM ratio at 1:4 level
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>memtier_builder_add_tier(builder, MEMKIND_REGULAR, <span style=color:#ae81ff>1</span>);
</span></span><span style=display:flex><span>memtier_builder_add_tier(builder, MEMKIND_DAX_KMEM, <span style=color:#ae81ff>4</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>struct</span> memtier_memory <span style=color:#f92672>*</span>m_tiered_memory <span style=color:#f92672>=</span>
</span></span><span style=display:flex><span>    memtier_builder_construct_memtier_memory(builder);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// allocate from lowest latency memory
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>void</span> <span style=color:#f92672>*</span>ptr1 <span style=color:#f92672>=</span> memkind_malloc(MEMKIND_LOWEST_LATENCY_LOCAL, <span style=color:#ae81ff>32</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// allocate from auto-tiered memory
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>void</span> <span style=color:#f92672>*</span>ptr2 <span style=color:#f92672>=</span> memtier_malloc(m_tiered_memory, <span style=color:#ae81ff>1024</span>);
</span></span></code></pre></div><h2 id=summary>Summary</h2><p>We have presented both the benefits and potential challenges of using
PMEM as a DRAM extension. However, the biggest drawback of explicit
tiering is the need to modify the application code. Therefore, in further blog notes,
we will describe some &ldquo;transparent tiering&rdquo; techniques and take a closer look
at the auto-tiering algorithms implemented in the Memkind library.</p><div class="tagcloud clearfix bottommargin"><a href=https://pmem.io/tags/tiering>tiering</a>
<a href=https://pmem.io/tags/memkind>memkind</a></div><div class=clear></div><div class="si-share border-0 d-flex justify-content-between align-items-center"><span>Share this Post:</span><div id=share-buttons><div class="social-icon si-borderless si-facebook" title="Share this on Facebook" onclick='window.open("http://www.facebook.com/share.php?u=https://pmem.io/blog/2022/06/memory-tiering-part-1/")'><i class=icon-facebook></i>
<i class=icon-facebook></i></div><div class="social-icon si-borderless si-twitter" title="Share this on Twitter" onclick='window.open("http://twitter.com/intent/tweet?text=Memory Tiering (part 1)&url=https://pmem.io/blog/2022/06/memory-tiering-part-1/")'><i class=icon-twitter></i>
<i class=icon-twitter></i></div><div class="social-icon si-borderless si-linkedin" title="Share this on Linkedin" onclick='window.open("https://www.linkedin.com/shareArticle?mini=true&url=https://pmem.io/blog/2022/06/memory-tiering-part-1/&title=&summary=&source=")'><i class=icon-linkedin></i>
<i class=icon-linkedin></i></div><div class="social-icon si-borderless si-pinterest" title="Share this on Pinterest" onclick='window.open("https://pinterest.com/pin/create/button/?url=&media=&description=")'><i class=icon-pinterest></i>
<i class=icon-pinterest></i></div><div class="social-icon si-borderless si-email3" title="Share this through Email" onclick='window.open("mailto:?&body=https://pmem.io/blog/2022/06/memory-tiering-part-1/")'><i class=icon-email3></i>
<i class=icon-email3></i></div></div></div></div></div><div class="row justify-content-between col-mb-30 post-navigation"><div class="col-12 col-md-auto text-center"><a href="https://pmem.io/blog/2022/06/memory-tiering-part-2-writing-transparent-tiering-solution/?ref=footer">&lArr; Memory Tiering (part 2):...</a></div><div class="col-12 col-md-auto text-center"><a href="https://pmem.io/blog/2022/06/similarity-search-opportunity-for-pmem/?ref=footer">Similarity Search -... &rArr;</a></div></div><div class=line></div><h4>Related Posts:</h4><div class="related-posts row posts-md col-mb-30"><div class="entry col-12 col-md-6"><div class="grid-inner row align-items-center gutter-20"><div class=col-4><div class=entry-image><a href=https://pmem.io/blog/2022/06/basic-asynchronous-hashmap-with-miniasync-library/ data-lightbox=image><img src=/images/pmem_logo.png alt="Basic asynchronous hashmap with Miniasync library"></a></div></div><div class=col-8><div class="entry-title title-xs"><h3><a href=https://pmem.io/blog/2022/06/basic-asynchronous-hashmap-with-miniasync-library/>Basic asynchronous hashmap with Miniasync library</a></h3></div><div class=entry-meta><ul><li><i class=icon-user></i> Krzysztof Święcicki</li><li><i class=icon-calendar3></i> 03 Jun, 2022</li></ul></div></div></div></div><div class="entry col-12 col-md-6"><div class="grid-inner row align-items-center gutter-20"><div class=col-4><div class=entry-image><a href=https://pmem.io/blog/2022/05/upcoming-asynchronous-interfaces-in-pmdk-libraries/ data-lightbox=image><img src=/images/pmem_logo.png alt="Upcoming asynchronous interfaces in PMDK libraries"></a></div></div><div class=col-8><div class="entry-title title-xs"><h3><a href=https://pmem.io/blog/2022/05/upcoming-asynchronous-interfaces-in-pmdk-libraries/>Upcoming asynchronous interfaces in PMDK libraries</a></h3></div><div class=entry-meta><ul><li><i class=icon-user></i> Piotr Balcer</li><li><i class=icon-calendar3></i> 11 May, 2022</li></ul></div></div></div></div><div class="entry col-12 col-md-6"><div class="grid-inner row align-items-center gutter-20"><div class=col-4><div class=entry-image><a href=https://pmem.io/blog/2022/02/leveraging-asynchronous-hardware-accelerators-for-fun-and-profit/ data-lightbox=image><img src=/images/pmem_logo.png alt="Leveraging asynchronous hardware accelerators for fun and profit"></a></div></div><div class=col-8><div class="entry-title title-xs"><h3><a href=https://pmem.io/blog/2022/02/leveraging-asynchronous-hardware-accelerators-for-fun-and-profit/>Leveraging asynchronous hardware accelerators for fun and profit</a></h3></div><div class=entry-meta><ul><li><i class=icon-user></i> Piotr Balcer</li><li><i class=icon-calendar3></i> 28 Feb, 2022</li></ul></div></div></div></div><div class="entry col-12 col-md-6"><div class="grid-inner row align-items-center gutter-20"><div class=col-4><div class=entry-image><a href=https://pmem.io/blog/2022/01/disaggregated-memory-in-pursuit-of-scale-and-efficiency/ data-lightbox=image><img src=/images/pmem_logo.png alt="Disaggregated Memory - In pursuit of scale and efficiency"></a></div></div><div class=col-8><div class="entry-title title-xs"><h3><a href=https://pmem.io/blog/2022/01/disaggregated-memory-in-pursuit-of-scale-and-efficiency/>Disaggregated Memory - In pursuit of scale and efficiency</a></h3></div><div class=entry-meta><ul><li><i class=icon-user></i> Piotr Balcer</li><li><i class=icon-calendar3></i> 21 Jan, 2022</li></ul></div></div></div></div></div></div></div><div class="sidebar col-lg-3"><div class=sidebar-widgets-wrap><div class="widget clearfix"><h4>Tag Cloud</h4><div class=tagcloud><a href=/tags/pmem class=block role=button>pmem</a>
<a href=/tags/persistent-memory class=block role=button>persistent-memory</a>
<a href=/tags/ndctl class=block role=button>ndctl</a>
<a href=/tags/pmdk class=block role=button>pmdk</a>
<a href=/tags/cxl class=block role=button>cxl</a>
<a href=/tags/daxctl class=block role=button>daxctl</a>
<a href=/tags/memkind class=block role=button>memkind</a>
<a href=/tags/async class=block role=button>async</a>
<a href=/tags/asynchronous class=block role=button>asynchronous</a>
<a href=/tags/concurrency class=block role=button>concurrency</a>
<a href=/tags/configure class=block role=button>configure</a>
<a href=/tags/dax class=block role=button>dax</a>
<a href=/tags/install class=block role=button>install</a>
<a href=/tags/intro class=block role=button>intro</a>
<a href=/tags/miniasync class=block role=button>miniasync</a>
<a href=/tags/setup class=block role=button>setup</a>
<a href=/tags/dml class=block role=button>dml</a>
<a href=/tags/dsa class=block role=button>dsa</a>
<a href=/tags/faq class=block role=button>faq</a>
<a href=/tags/imdb class=block role=button>imdb</a>
<a href=/tags/memory class=block role=button>memory</a>
<a href=/tags/pmem-use-case class=block role=button>pmem-use-case</a>
<a href=/tags/pmem2 class=block role=button>pmem2</a>
<a href=/tags/sanitize class=block role=button>sanitize</a>
<a href=/tags/secure-erase class=block role=button>secure-erase</a>
<a href=/tags/sql class=block role=button>sql</a>
<a href=/tags/tiering class=block role=button>tiering</a>
<a href=/tags/2019 class=block role=button>2019</a>
<a href=/tags/blogs class=block role=button>blogs</a>
<a href=/tags/crash class=block role=button>crash</a></div></div></div></div></div></div></div></section><footer id=footer class="border-0 bg-white"><div id=copyrights><div class="container clearfix"><div class="row justify-content-between col-mb-30"><div class="col-12 col-lg-auto text-center text-lg-start"><div id=logo><a href=/ class=standard-logo data-dark-logo=images/logo-dark.png><img src=https://pmem.io/images/pmem_logo.png alt="PMem Logo"></a>
<a href=/ class=retina-logo data-dark-logo=images/logo-dark@2x.png><img src=https://pmem.io/images/pmem_logo.png alt="PMem Logo"></a></div></div><div class="col-12 col-lg-auto text-center text-lg-end"><div class="copyrights-menu copyright-links clearfix text-uppercase"><a href=https://pmem.io/about>about</a>/
<a href=https://pmem.io/blog>blog</a>/
<a href=https://pmem.io/community>community</a>/
<a href=https://pmem.io/cookies.html>Cookies</a>/
<a href=https://pmem.io/developer-hub>developer Hub</a>/
<a href=https://pmem.io/learn>learn</a>/
<a href=https://pmem.io/privacy.html>Privacy</a>/
<a href=https://pmem.io/solutions>solutions</a>/
<a href=https://pmem.io/terms.html>Terms</a></div><div class="col-lg-auto text-center mt-0"><p>Copyright &copy; 2023 pmem.io</p></div></div></div></div></div></footer></div><div id=gotoTop class=icon-angle-up></div><script src=/js/jquery.js></script>
<script src=/js/plugins.min.js></script>
<script src=/js/custom.js></script>
<script src=/js/darkmode.js></script>
<script src=/js/functions.js></script>
<script type=text/javascript src="https://ui.customsearch.ai/api/ux/rendering-js?customConfig=011a90aa-26ea-46b5-bf60-4b5b407c72c6&market=en-US&version=latest&q="></script></body></html>