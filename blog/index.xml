<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog on PMem.io</title><link>https://pmem.io/blog/</link><description>Recent content in Blog on PMem.io</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Thu, 25 May 2023 13:46:31 +0200</lastBuildDate><atom:link href="https://pmem.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring the Software Ecosystem for Compute Express Link (CXL) Memory</title><link>https://pmem.io/blog/2023/05/exploring-the-software-ecosystem-for-compute-express-link-cxl-memory/</link><pubDate>Thu, 25 May 2023 13:46:31 +0200</pubDate><guid>https://pmem.io/blog/2023/05/exploring-the-software-ecosystem-for-compute-express-link-cxl-memory/</guid><description>CXL Software ecosystem The Compute Express Link (CXL) is going to be a transformative new technology in the heterogeneous memory space. While the transition from Persistent Memory (PMem) to CXL.mem may seem challenging at first, developers who have optimized their applications for PMem will find that no significant changes may be required. In this article, we will explore the CXL software ecosystem and its compatibility with the established PMem concepts and libraries.</description></item><item><title>Dockers usage in PMDK</title><link>https://pmem.io/blog/2022/12/dockers-usage-in-pmdk/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/12/dockers-usage-in-pmdk/</guid><description>In this blog post, I&amp;rsquo;ll describe why we believe dockers are easy to use, time-saving, and valuable for day-to-day programming and debugging. If you have never heard of dockers (or containers in general), please read, for example, this overview. We use dockers in almost all of the repositories in our organization. In this blog post, I will describe how we use dockers based on the PMDK repository. In some of our repositories, like in memkind, we use a bit different approach, but it still relies on docker.</description></item><item><title>Update on PMDK and our long term support strategy</title><link>https://pmem.io/blog/2022/11/update-on-pmdk-and-our-long-term-support-strategy/</link><pubDate>Wed, 16 Nov 2022 00:00:00 +0200</pubDate><guid>https://pmem.io/blog/2022/11/update-on-pmdk-and-our-long-term-support-strategy/</guid><description>Following Intel’s Q2 earnings call announcing the wind-down of Intel’s Optane business, Intel will also be winding down its investment in new feature development on Persistent Memory Development Kit (PMDK) libraries and adjusting long-term support and maintenance plans accordingly.
The PMDK collection of open-source libraries and tools hosted on GitHub will continue to be available to the software community. Documentation and resources via the pmem.io website will also remain available.</description></item><item><title>Introduction to libpmem2 (part 1)</title><link>https://pmem.io/blog/2022/06/introduction-to-libpmem2-part-1/</link><pubDate>Thu, 30 Jun 2022 10:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/06/introduction-to-libpmem2-part-1/</guid><description>A new beginning This blog post will be about the recently created library libpmem2, which is a part of PMDK repository. Before we go into details it&amp;rsquo;s worth having a short look at the history. About seven years ago, the first persistent memory library known as libpmem was started. The concept of persistent memory was becoming a reality. Initially, libpmem provided simple support for memory memory management on pmem. Over time, the development of hardware, software, as well as changing customer needs made it necessary to evolve the library to include new features such as support for Windows, DevDax, RAS, eADR, and others.</description></item><item><title>Memory Tiering (part 2): Writing Transparent Tiering Solution</title><link>https://pmem.io/blog/2022/06/memory-tiering-part-2-writing-transparent-tiering-solution/</link><pubDate>Wed, 29 Jun 2022 00:00:00 +0200</pubDate><guid>https://pmem.io/blog/2022/06/memory-tiering-part-2-writing-transparent-tiering-solution/</guid><description>This is the second part of the series of articles about memory tiering. The first one explained what memory tiering is and why we need it, the second one will explain some mechanisms behind transparent tiering.
This article is intended for those who would like to learn how tiering/numa balancing or memory profiling solutions work under the hood. We will focus on a high-level overview of how tiering/numa balancing could be designed, instead of dissecting any particular solution.</description></item><item><title>Memory Tiering (part 1)</title><link>https://pmem.io/blog/2022/06/memory-tiering-part-1/</link><pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/06/memory-tiering-part-1/</guid><description>Extending memory capacity with PMEM Databases such as Redis (an in-memory key-value open-source database) consume a lot of memory. Since fast access is essential for them, they use DRAM to store their data. DRAM is quite expensive and has limited capacity, so a solution we propose in this blog post is to use PMEM (and in the future other types of memory available through CXL - see a pmem.io blog post about it).</description></item><item><title>Similarity Search - opportunity for PMEM</title><link>https://pmem.io/blog/2022/06/similarity-search-opportunity-for-pmem/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0200</pubDate><guid>https://pmem.io/blog/2022/06/similarity-search-opportunity-for-pmem/</guid><description>At a high level, computing solves problems. These problems, even though different and individual, are sometimes somehow related. A new algorithmic challenge can usually be solved by bringing it down to a well-defined problem with an existing solution. Today, we will be talking about one of such universal solutions - similarity search, which has found its application in various areas of life, from search engines that tell us what we want to find, through recommendation engines that tell us what to watch, where to eat and what to buy, all the way to data science that provides valuable input to business decisions.</description></item><item><title>Basic asynchronous hashmap with Miniasync library</title><link>https://pmem.io/blog/2022/06/basic-asynchronous-hashmap-with-miniasync-library/</link><pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/06/basic-asynchronous-hashmap-with-miniasync-library/</guid><description>Miniasync library provides a framework for the composition and execution of asynchronous tasks in C. To accommodate different user-defined tasks and various types of data that they take in, libminiasync makes use of macros.
Using libminiasync for the first time can be challenging. There are multiple examples on the miniasync repo to make it easier. One of them is a hashmap example.
Hashmap example overview The hashmap example on the Miniasync repository presents a hashmap with a fixed size that allocates memory upon key-value pair insertion.</description></item><item><title>Upcoming asynchronous interfaces in PMDK libraries</title><link>https://pmem.io/blog/2022/05/upcoming-asynchronous-interfaces-in-pmdk-libraries/</link><pubDate>Wed, 11 May 2022 10:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/05/upcoming-asynchronous-interfaces-in-pmdk-libraries/</guid><description>In the previous article, I wrote about a new upcoming Xeon platform feature, Data Streaming Accelerator (DSA) - a memory-to-memory DMA engine, and what opportunities and challenges it presents. I outlined the approach we are taking in Persistent Memory Development Kit (PMDK) to expose asynchronous APIs that can take advantage of this new hardware. Lastly, I introduced libminiasync, which is our framework for abstracting asynchronous operations.
This time, I will discuss how miniasync is being used in libpmem2 and our plans for its integration into the rest of PMDK libraries.</description></item><item><title>Leveraging asynchronous hardware accelerators for fun and profit</title><link>https://pmem.io/blog/2022/02/leveraging-asynchronous-hardware-accelerators-for-fun-and-profit/</link><pubDate>Mon, 28 Feb 2022 10:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/02/leveraging-asynchronous-hardware-accelerators-for-fun-and-profit/</guid><description>One of the greatest benefits of Persistent Memory is that it&amp;rsquo;s directly accessible by the CPU. But that can also be one of its downsides for specific use cases. For example, if you want to use PMem as an ultra-fast storage device with low access latency.
PMem as storage impedance mismatch The reason for that is simple - block storage I/O is typically asynchronous due to its relatively high latency and high queue depths required to reach optimal throughputs.</description></item><item><title>Introduction to pmemstream</title><link>https://pmem.io/blog/2022/01/introduction-to-pmemstream/</link><pubDate>Wed, 26 Jan 2022 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2022/01/introduction-to-pmemstream/</guid><description>What is pmemstream? Libpmemstream implements a pmem-optimized log data structure and provides stream-like access to data. It presents a contiguous logical address space, divided into regions, with log entries of arbitrary size. We intend for this library to be a foundation for various, more complex higher-level solutions. Read on to learn about a few example use cases we have in mind. Like most libraries in the PMDK family, this one also focuses on delivering a generic, easy-to-use set of functions.</description></item><item><title>Disaggregated Memory - In pursuit of scale and efficiency</title><link>https://pmem.io/blog/2022/01/disaggregated-memory-in-pursuit-of-scale-and-efficiency/</link><pubDate>Fri, 21 Jan 2022 10:00:00 +0100</pubDate><guid>https://pmem.io/blog/2022/01/disaggregated-memory-in-pursuit-of-scale-and-efficiency/</guid><description>A software person perspective on new upcoming interconnect technologies.
Existing Server Landscape Servers are expensive. And difficult to maintain properly. That&amp;rsquo;s why most people turn to the public cloud for their hosting and computing needs. Dynamic virtual server instances have been key to unlocking efficiency gains for both Cloud Service Providers (CSPs) and their users. CSPs can leverage virtualization to colocate many workloads on fewer physical servers. And cloud users have access to a huge pool of on-demand processing power, only having to pay for what they use.</description></item><item><title>How to Contribute to pmem.io</title><link>https://pmem.io/blog/2021/10/how-to-contribute-to-pmem.io/</link><pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2021/10/how-to-contribute-to-pmem.io/</guid><description>Contributing to this repository [This entry was edited on 2022-07-22 to update Code of Conduct link and add a link to Contributing guide] Please note that this blog post is a snapshot of our CONTRIBUTING file. For always up-to-date information, please see guideline file on repository with content of this website.
Getting started Before you begin:
The pmem.io website is powered by the Hugo static site generator and hosted on GitHub Pages.</description></item><item><title>Libpmemobj-cpp - lessons learned</title><link>https://pmem.io/blog/2021/09/libpmemobj-cpp-lessons-learned/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2021/09/libpmemobj-cpp-lessons-learned/</guid><description>Introduction We&amp;rsquo;ve been working on C++ bindings for libpmemobj since around 2016 - see our very first tutorial for libpmemobj-cpp. We&amp;rsquo;ve come a long way since then. A lot has changed - we&amp;rsquo;ve gained more experience and knowledge, added new features, fixed quite a few bugs, and created at least half a dozen new containers. It&amp;rsquo;s fair to state this product is now far more mature and well-developed. Over time, we&amp;rsquo;ve learned at least several lessons about designing and overcoming issues in C++ applications for persistent memory.</description></item><item><title>Concurrency considerations in libpmemobj-cpp</title><link>https://pmem.io/blog/2021/09/concurrency-considerations-in-libpmemobj-cpp/</link><pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate><guid>https://pmem.io/blog/2021/09/concurrency-considerations-in-libpmemobj-cpp/</guid><description>Introduction Ensuring data consistency on pmem is a challenging task. It gets even more complicated if data is modified concurrently. This blog post describes several challenges related to data visibility, using transactions in multi-threaded environments, and memory leaks.
Lock-free programming on pmem A fundamental issue (if eADR is not used) is data visibility. When a thread issues a temporal (e.g., MOV) store instruction, the modification might be visible to other threads before it is persistent (data can still be in a CPU cache).</description></item><item><title>Memkind support for heterogeneous memory attributes</title><link>https://pmem.io/blog/2021/05/memkind-support-for-heterogeneous-memory-attributes/</link><pubDate>Wed, 12 May 2021 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2021/05/memkind-support-for-heterogeneous-memory-attributes/</guid><description>Introduction Memkind is a library mostly associated with enabling Persistent Memory. However, it is not the only type of memory that memkind supports. The library is a general solution designed for platforms with heterogeneous memory.
But before we delve into heterogeneous memory itself, let&amp;rsquo;s start with a short recap about NUMA. The NUMA concept solved the problem of a dynamic extension of the CPU count per socket and system memory. Before NUMA, Uniform Memory Access (UMA) was a common model, in which all processors shared physical memory uniformly.</description></item><item><title>Using Memkind in Hazelcast</title><link>https://pmem.io/blog/2021/02/using-memkind-in-hazelcast/</link><pubDate>Thu, 11 Feb 2021 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2021/02/using-memkind-in-hazelcast/</guid><description>This blog post is published on the Hazelcast blog as well. If interested in Hazelcast, check the other posts there too.
Introduction The mission of the PMDK team has always been and will always be to make programming persistent memory easier for the community of software developers. One of our goals is to help simplify the integration of persistent memory into software solutions by making it transparent as possible. Adopting ground-breaking and disruptive technology creates a chasm, which is challenging to cross at first.</description></item><item><title>API overview of pmemkv-java binding</title><link>https://pmem.io/blog/2020/10/api-overview-of-pmemkv-java-binding/</link><pubDate>Fri, 30 Oct 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/10/api-overview-of-pmemkv-java-binding/</guid><description>Pmemkv is a key-value data store written in C and C++, however, it also opens up a way to leverage persistent memory by developers who prefer high-level languages - such as Java. For more information about other bindings please read Language bindings for pmemkv article and pmemkv README
We built an API for pmemkv-java binding on top of libpmemkv 1.0 API, but java binding is also compatible with newer versions of libpmemkv.</description></item><item><title>TieredMemDB - Redis with Persistent Memory</title><link>https://pmem.io/blog/2020/09/tieredmemdb-redis-with-persistent-memory/</link><pubDate>Fri, 25 Sep 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/09/tieredmemdb-redis-with-persistent-memory/</guid><description>[14-Jan-2022 Note: this blog post has been updated due to the renaming of our database from MemKeyDB to TieredMemDB] Context Redis is an in-memory database that supports various data-structures and stores them in main memory. To support data durability, Redis relies on creating periodical snapshots of data or logging all commands that reach the server.
When Persistent Memory was first introduced, we&amp;rsquo;ve started working on various approaches of using it in Redis.</description></item><item><title>Static code analysis of the PMDK</title><link>https://pmem.io/blog/2020/08/static-code-analysis-of-the-pmdk/</link><pubDate>Thu, 20 Aug 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/08/static-code-analysis-of-the-pmdk/</guid><description>Introduction In the PMDK team, we focus on the quality of our codebase. One of the standard practices in the software development is a static code analysis, which improves the overall project quality and fixes bugs in the early stage of development. Since there is no silver bullet for avoiding bugs, we already use two different static analysis tools and many runtime checkers e.g. valgrind. Improving static analysis effectiveness is a separate academic problem.</description></item><item><title>Introduction to LLPL</title><link>https://pmem.io/blog/2020/05/introduction-to-llpl/</link><pubDate>Wed, 27 May 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/05/introduction-to-llpl/</guid><description>There are several ways to program with persistent memory from Java. A recent pmem.io blog article described the pmemkv library, a persistent key-value store, which can be used from multiple languages. For Java, pmemkv supports three ubiquitous Java types: String, byte[], and ByteBuffer.
Another PMDK component, the Low-Level Persistence Library (LLPL) is an open source Java library (https://github.com/pmem/llpl) that gives Java developers access to persistent memory in a very fast and flexible way.</description></item><item><title>300 nanoseconds (2 of 2)</title><link>https://pmem.io/blog/2020/03/300-nanoseconds-2-of-2/</link><pubDate>Thu, 26 Mar 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/03/300-nanoseconds-2-of-2/</guid><description>At the end of the first part of this blog series, I posed three fundamental questions regarding the design of failure atomic data structures for persistent memory.
What does it mean to allocate persistent memory? How to do fail-safe atomic updates? Are all data structures suitable for persistent memory? This time around, I will try to answer these questions to the best of my ability. We will return to the doubly-linked list example to see how it can be modified for PMEM.</description></item><item><title>Language bindings for pmemkv</title><link>https://pmem.io/blog/2020/03/language-bindings-for-pmemkv/</link><pubDate>Wed, 04 Mar 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/03/language-bindings-for-pmemkv/</guid><description>pmemkv is a local/embedded key-value datastore optimized for persistent memory. It is written in C and C++, but to satisfy a wider audience it comes with several bindings for high-level languages. Currently: Java (with JNI), Node.js, Python and Ruby.
The picture below illustrates architecture and software stack of pmemkv and its bindings. The most up-to-date information about pmemkv and its bindings is located in pmemkv&amp;rsquo;s README file.
Common for bindings There are few common characteristics of all bindings:</description></item><item><title>Introduction to libmemkind</title><link>https://pmem.io/blog/2020/01/introduction-to-libmemkind/</link><pubDate>Mon, 20 Jan 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/01/introduction-to-libmemkind/</guid><description>Introduction Memkind is the library that simplify usage of persistent memory in a volatile mode. There are NVDIMMs technologies, such as Intel Optane DCPMM, that provides persistency, byte-addressability, and also a high capacity when compared with DRAM modules. They can be used as an expansion of main memory and utilized by applications which consume a large amount of memory and do not require persistency, such as in-memory databases, caching engines and scientific simulations.</description></item><item><title>Memkind support for KMEM DAX option</title><link>https://pmem.io/blog/2020/01/memkind-support-for-kmem-dax-option/</link><pubDate>Mon, 20 Jan 2020 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2020/01/memkind-support-for-kmem-dax-option/</guid><description>Introduction Linux kernel version 5.1 brings in support for the volatile-use of persistent memory as a hotplugged memory region (KMEM DAX). When this feature is enabled, persistent memory is seen as a separate memory-only NUMA node(s). libmemkind API was extended to include new kinds that allow for automatic detection and allocation from these new persistent memory NUMA nodes.
Requirements 1. Kernel 5.1 with KMEM DAX driver enabled.
If support of KMEM DAX driver isn&amp;rsquo;t enabled in your kernel you will have to configure proper driver installation by run nconfig and enable driver.</description></item><item><title>300 nanoseconds (1 of 2)</title><link>https://pmem.io/blog/2019/12/300-nanoseconds-1-of-2/</link><pubDate>Thu, 19 Dec 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/12/300-nanoseconds-1-of-2/</guid><description>Educating people has always been a challenge for me. I tend to skip over things I wrongly consider obvious, or do large leaps in reasoning when explaining a solution to a problem. And so, when faced with an attempt to explain a complex topic, I tend to ramble on and on, hoping that the audience knows when to interrupt me if I go too fast. However, this doesn&amp;rsquo;t hold true for blog posts, such as the one I&amp;rsquo;m currently writing.</description></item><item><title>Vmem is split out of PMDK</title><link>https://pmem.io/blog/2019/10/vmem-is-split-out-of-pmdk/</link><pubDate>Thu, 31 Oct 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/10/vmem-is-split-out-of-pmdk/</guid><description>Introduction We have just split libvmem and its companion libvmmalloc out of the PMDK tree. They now live in a separate repository, and will follow their own release cadence. And, as these libraries are considered mature and finished, no new releases are planned once the split has been tested and tagged &amp;ndash; except for defects and new requirements of underlying platforms.
Further development libvmem remains the only way to use filesystem-managed persistent memory for volatile allocations on Windows.</description></item><item><title>New release of PMDK</title><link>https://pmem.io/blog/2019/10/new-release-of-pmdk/</link><pubDate>Fri, 11 Oct 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/10/new-release-of-pmdk/</guid><description>If you are following our mailing group, you&amp;rsquo;ve probably noticed a stream of release announcements for libraries that are a part of PMDK. Here&amp;rsquo;s a recap of the most important new features and additions.
libpmemkv 1.0 The primary goal of PMDK is enabling adoption of Persistent Memory. We do so by creating the building blocks that applications can utilize to support PMEM. So far, our work was mostly concentrated on important base functionality such as memory allocation or transactions, and only recently we&amp;rsquo;ve started to build on that foundation with C++ containers - making persistent memory programming easier and easier.</description></item><item><title>C++ standard limitations and Persistent Memory</title><link>https://pmem.io/blog/2019/10/c-standard-limitations-and-persistent-memory/</link><pubDate>Fri, 04 Oct 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/10/c-standard-limitations-and-persistent-memory/</guid><description>Introduction C++ language restrictions and the persistent memory programming paradigm imply serious restrictions on objects which may be stored on persistent medium. A user can access persistent memory with memory mapped files to take advantage of its byte addressability thanks to libpmemobj and Storage Networking Industry Association non-volatile memory programming model. No serialization takes place here, thus applications must be able to read and modify directly from the medium even after application was closed and reopened or after the event of power loss.</description></item><item><title>Multi-level vmemcache</title><link>https://pmem.io/blog/2019/06/multi-level-vmemcache/</link><pubDate>Wed, 12 Jun 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/06/multi-level-vmemcache/</guid><description>Introduction vmemcache which we have recently described performs close to optimum when either all keys are approximately equally likely to be queried, or when all key:value pairs fit completely into the fastest form of memory you are willing to use. But, in many workloads, some keys are “hot” and queried over and over again, while the rest, “cold”, may comfortably reside on slower medium. This calls for multiple linked instances of vmemcache, each residing in a different tier.</description></item><item><title>libvmemcache - buffer-based LRU cache</title><link>https://pmem.io/blog/2019/05/libvmemcache-buffer-based-lru-cache/</link><pubDate>Tue, 07 May 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/05/libvmemcache-buffer-based-lru-cache/</guid><description>Introduction libvmemcache is a volatile key-value store optimized for operating on NVDIMM based space. However, it can work with any filesystem whether it is stored in memory (tmpfs) or on any storage device. Consequently, libvmemcache will be significantly less performant if it is stored on the storage device other than NVDIMMs.
libvmemcache is an embeddable and lightweight in-memory caching solution. It is designed to fully take advantage of large capacity memory, such as persistent memory with DAX through memory mapping in an efficient and scalable way.</description></item><item><title>C++ persistent containers - vector</title><link>https://pmem.io/blog/2019/02/c-persistent-containers-vector/</link><pubDate>Wed, 20 Feb 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/02/c-persistent-containers-vector/</guid><description>[Note: pmem::obj::vector&amp;lt;&amp;gt; is no longer experimental. The rest of the information in this blog post is still accurate.] Introduction The main idea behind pmem containers is to fully exploit persistent memory potential by designing optimized on-media layouts and algorithms for persistent memory programming. On November, we published a blog post about pmem containers. If you haven’t read it yet, I encourage you to do that now.
We have recently added pmem::obj:vector container to libpmemobj-cpp library.</description></item><item><title>Pmreorder basics</title><link>https://pmem.io/blog/2019/02/pmreorder-basics/</link><pubDate>Mon, 04 Feb 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/02/pmreorder-basics/</guid><description>Introduction It&amp;rsquo;s good practice to run persistent memory application under pmemcheck - a tool which is described here and here.
In this post, we are going to learn about another tool for persistence correctness checking. As you might already know if you&amp;rsquo;ve read posts linked above, pmemcheck verifies if all stores are made persistent in a proper manner. Our new tool, pmreorder, extends this functionality. It traverses the sequences of stores between flush-fence barriers made by the application, and then replays these memory operations many times in different combinations, to simulate the various possible ways the stores to the NVDIMM could be ordered by the system.</description></item><item><title>Pool conversion tool</title><link>https://pmem.io/blog/2019/02/pool-conversion-tool/</link><pubDate>Fri, 01 Feb 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/02/pool-conversion-tool/</guid><description>Introduction When we published the first PMDK stable release, we committed to maintaining stable on-media layout. This means that all future PMDK changes have to be backward compatible. Unfortunately, we weren&amp;rsquo;t successful in adhering to the strict requirements which would be needed to maintain compatibility, mostly because we made changes whose benefit far outweighed the costs. For this reason, we created the pmempool convert command. This tool was used to convert pools which were created with old PMDK versions to the newer on-media layout.</description></item><item><title>Extended memcpy in PMDK 1.5</title><link>https://pmem.io/blog/2019/01/extended-memcpy-in-pmdk-1.5/</link><pubDate>Tue, 22 Jan 2019 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2019/01/extended-memcpy-in-pmdk-1.5/</guid><description>In PMDK 1.5 we added new APIs for bulk persistent memory modifications. In short, we did this to:
give applications the ability to perform low-level performance optimizations clean up the naming scheme In order to understand what exactly and why we did that, let&amp;rsquo;s review the old API. In PMDK 1.4 we had these functions:
void *pmem_memmove_persist(void *pmemdest, const void *src, size_t len); void *pmem_memcpy_persist (void *pmemdest, const void *src, size_t len); void *pmem_memset_persist (void *pmemdest, int c, size_t len); void *pmem_memmove_nodrain(void *pmemdest, const void *src, size_t len); void *pmem_memcpy_nodrain (void *pmemdest, const void *src, size_t len); void *pmem_memset_nodrain (void *pmemdest, int c, size_t len); void *pmemobj_memcpy_persist(PMEMobjpool *pop, void *pmemdest, const void *src, size_t len); void *pmemobj_memset_persist(PMEMobjpool *pop, void *pmemdest, int c, size_t len); As you can see, there are two variants of each API - one with _persist and another one with _nodrain suffix.</description></item><item><title>Pool features</title><link>https://pmem.io/blog/2018/12/pool-features/</link><pubDate>Wed, 05 Dec 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/12/pool-features/</guid><description>Introduction Since the very first release, PMDK pools had internal feature flags. They were mostly a hidden implementation detail. The more observant users might have noticed pool features listing in pmempool info output, but that&amp;rsquo;s about it.
Release 1.5 introduced a set of new feature flags. And since it&amp;rsquo;s imperative that system administrators have the ability to manage enabled features, we&amp;rsquo;ve added functionality that helps with that:
pmempool tool commands for managing features libpmempool API for managing features CTL namespace for setting a SHUTDOWN_STATE initial value Taxonomy of pool features Each of the toggleable features can be disabled or enabled for the given pool and the support for these features might vary depending on the library version.</description></item><item><title>Bad blocks</title><link>https://pmem.io/blog/2018/11/bad-blocks/</link><pubDate>Mon, 26 Nov 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/11/bad-blocks/</guid><description>Introduction Over time, storage devices can acquire uncorrectable media errors often called &amp;ldquo;bad blocks&amp;rdquo;. A bad block is a part of a storage media that is either inaccessible or unwritable due to a permanent physical damage. In case of memory mapped I/O, if a process tries to access (read or write) the corrupted block, it will be terminated by the SIGBUS signal.
Handling bad blocks in PMDK libraries PMDK libraries can handle bad blocks if the CHECK_BAD_BLOCKS compat feature is turned on.</description></item><item><title>C++ persistent containers</title><link>https://pmem.io/blog/2018/11/c-persistent-containers/</link><pubDate>Tue, 20 Nov 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/11/c-persistent-containers/</guid><description>PMEM containers Our goal for the libpmemobj C++ bindings is to create a friendly and less error prone API for persistent memory programming. Even with persistent memory pool allocators, convenient interface for creating and managing transactions, auto-snapshotting class templates and smart persistent pointers, designing an application with persistent memory usage may still prove challenging without a plethora of niceties that the C++ programmers are used to.
The natural step forward to make persistent programming easier, is to provide programmers with efficient and useful containers.</description></item><item><title>C++ persistent containers - array</title><link>https://pmem.io/blog/2018/11/c-persistent-containers-array/</link><pubDate>Fri, 02 Nov 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/11/c-persistent-containers-array/</guid><description>[Note: pmem::obj::array&amp;lt;&amp;gt; is no longer experimental. The rest of the information in this blog post is still accurate.] Introduction Until now, our C++ bindings were missing one important component - persistent containers. In 1.5 release we have introduced the first one - pmem::obj::array. This container is currently placed in experimental namespace and folder (this means that both API and layout can change). It has almost the same functionality as std::array from C++11 but takes care of adding elements to a transaction.</description></item><item><title>New release of PMDK</title><link>https://pmem.io/blog/2018/10/new-release-of-pmdk/</link><pubDate>Mon, 22 Oct 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/10/new-release-of-pmdk/</guid><description>We&amp;rsquo;ve been very quiet on this blog as of late, mostly because of the amount of work that we needed to put into our very ambitiously planned 1.5 release. But we&amp;rsquo;ve made it, and there&amp;rsquo;s finally time to get back to discussing the technical minutiae of our work. In this post, we will go over the major library changes that have been introduced in 1.5.
Release planning in the open But first, I&amp;rsquo;d like to highlight a change to our release planning process.</description></item><item><title>Running FIO with pmem engines</title><link>https://pmem.io/blog/2018/06/running-fio-with-pmem-engines/</link><pubDate>Mon, 25 Jun 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/06/running-fio-with-pmem-engines/</guid><description>When we, the PMDK team, want to check performance of our library, either to see if there was any regression or if our tweaks did a good job, we run benchmarks. One of them is FIO. It helps us simulate synthetic traffic of reads and writes to a pmem device. In this blog post I will introduce this tool and explain how we commonly use it.
Preparing the environment For starters, links to needed software:</description></item><item><title>Using Persistent Memory Devices with the Linux Device Mapper</title><link>https://pmem.io/blog/2018/05/using-persistent-memory-devices-with-the-linux-device-mapper/</link><pubDate>Tue, 15 May 2018 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2018/05/using-persistent-memory-devices-with-the-linux-device-mapper/</guid><description>Introduction X86/X64 systems do not typically interleave Persistent Memory Devices (also referred to as &amp;lsquo;modules&amp;rsquo; or &amp;lsquo;DIMMs&amp;rsquo;) across sockets, so a two-socket system will have two separate interleave sets. To use these interleave sets as a single device requires using a software device mapper or volume manager.
This article focuses on using the &amp;lsquo;striped&amp;rsquo; (dm-stripe) and &amp;rsquo;linear&amp;rsquo; (dm-linear) target drivers with persistent memory devices to create virtual devices on which direct access (DAX) enabled filesystems can be created.</description></item><item><title>Benchmarking with different storage engines using pmemkv</title><link>https://pmem.io/blog/2017/12/benchmarking-with-different-storage-engines-using-pmemkv/</link><pubDate>Wed, 27 Dec 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/12/benchmarking-with-different-storage-engines-using-pmemkv/</guid><description>We&amp;rsquo;re closing out 2017 with two big improvements to pmemkv: support for multiple storage engines, and an improved benchmarking utility based on db_bench. These changes set the stage for some interesting experiments to come next year, as we continue to add new features and tune performance of pmemkv and its utilities and bindings.
Multiple storage engines A new virtual interface (KVEngine) was recently introduced that allows pmemkv to provide multiple storage engine implementations, without changes to utilities or language bindings or applications using the pmemkv API.</description></item><item><title>Announcing the Persistent Memory Development Kit</title><link>https://pmem.io/blog/2017/12/announcing-the-persistent-memory-development-kit/</link><pubDate>Mon, 11 Dec 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/12/announcing-the-persistent-memory-development-kit/</guid><description>This is to announce a name change: The NVML project is now known as PMDK, the Persistent Memory Development Kit.
Why the name change? The old name, NVML, made it sound like the project produced a single library that applied to Non-Volatile Memory. In reality, the project currently supports ten libraries, targeted at various use cases for persistent memory, along with language support for C, C++, Java, and Python, tools like the pmemcheck plug-in for valgrind, and an increasing body of documentation, code examples, tutorials, and blog entries.</description></item><item><title>Progress Report Q3 2017</title><link>https://pmem.io/blog/2017/10/progress-report-q3-2017/</link><pubDate>Thu, 05 Oct 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/10/progress-report-q3-2017/</guid><description>The last quarter was rather&amp;hellip; peaceful. But nevertheless there were a few noteworthy things.
FreeBSD &amp;amp; ARM We always asserted that our library is multi-platform and hardware agnostic&amp;hellip; as long as your platform is a recent distribution of Linux (or Windows) on x86 hardware :)
Two things happened that intend to change the current status quo:
There&amp;rsquo;s an active and ongoing effort of porting the linux-specific parts of the library to FreeBSD, mostly led by @gaweinbergi.</description></item><item><title>Using Standard Library Containers with Persistent Memory</title><link>https://pmem.io/blog/2017/07/using-standard-library-containers-with-persistent-memory/</link><pubDate>Mon, 10 Jul 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/07/using-standard-library-containers-with-persistent-memory/</guid><description>Introduction Somewhere along the road, when we were doing the C++ bindings for libpmemobj, we found the need for some kind of containers. We were faced with two viable solutions: write everything from scratch or adapt an existing implementation. The obvious choice was NOT to implement from scratch. We would have to implement at least the basic containers from the C++ standard: vector, list, set, map and their multi- companions. That would be a lot of work, not to mention the testing and maintenance effort.</description></item><item><title>Progress Report Q2 2017</title><link>https://pmem.io/blog/2017/07/progress-report-q2-2017/</link><pubDate>Tue, 04 Jul 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/07/progress-report-q2-2017/</guid><description>It&amp;rsquo;s summer already (at least in my hemisphere) ! :) And it&amp;rsquo;s time for the next progress report.
Control interface After a very long in the oven, we&amp;rsquo;ve finally finalized and merged the CTL API which allows for introspection and modification of the internal state of the library.
This feature has been shaped after the mallctl() available in jemalloc.
The way it works is very simple. Developer defines a function-like entry point, decides whether or not the entry point allows for reading, writing or both, and finally specifies the argument type.</description></item><item><title>Apache Kudu Persistent Memory Enabled Block Cache</title><link>https://pmem.io/blog/2017/04/apache-kudu-persistent-memory-enabled-block-cache/</link><pubDate>Mon, 03 Apr 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/04/apache-kudu-persistent-memory-enabled-block-cache/</guid><description>Using NVM Libraries To Add Persistent Memory Support to Apache Kudu Block Cache An early project done with the NVM libraries was adding persistent memory support, both volatile and persistent mode, into the Apache Kudu storage engine block cache. This project required modification of existing code.
Apache Kudu: https://github.com/apache/kudu
My repository with the modified code: https://github.com/sarahjelinek/kudu, branch: sarah_kudu_pmem
The volatile mode support for persistent memory has been fully integrated into the Kudu source base.</description></item><item><title>Progress Report Q1 2017</title><link>https://pmem.io/blog/2017/03/progress-report-q1-2017/</link><pubDate>Wed, 29 Mar 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/03/progress-report-q1-2017/</guid><description>It&amp;rsquo;s been three months already since the last time I wrote something ;) Time really flies by quickly when you are doing interesting stuff.
We&amp;rsquo;ve been very busy with lots of improvements to the library as well as A LOT of tiny fixes: over the last three months the team eliminated virtually every bug found by various static analysis tools and valgrind.
But no-one wants to hear about that, so here&amp;rsquo;s the meaty stuff:</description></item><item><title>Zero-copy leaf splits in pmemkv</title><link>https://pmem.io/blog/2017/03/zero-copy-leaf-splits-in-pmemkv/</link><pubDate>Thu, 09 Mar 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/03/zero-copy-leaf-splits-in-pmemkv/</guid><description>In a B+ tree, splitting a full leaf into two leaves is one of its slowest operations, but pmemkv optimizes this using a zero-copy strategy. Rather then copying any key/value data between full and new leaf, pmemkv splits leaves by swapping persistent structures in place. This minimizes write amplification and increases performance compared to copying, especially for larger key/value sizes.
The diagram below illustrates a persistent leaf being split. A &amp;lsquo;slot&amp;rsquo; is the term used in pmemkv for the grouping of a key, the Pearson hash for the key, and the value for the key.</description></item><item><title>Introducing pmemkv</title><link>https://pmem.io/blog/2017/02/introducing-pmemkv/</link><pubDate>Tue, 21 Feb 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/02/introducing-pmemkv/</guid><description>We&amp;rsquo;ve blogged before about building and optimizing key-value stores for persistent memory, and we&amp;rsquo;re excited to put these ideas to the test in a more formal way.
Our new pmemkv project is an open-source key-value store that is optimized for read-heavy workloads on persistent memory. Compared with key-value stores based on the LSM algorithm, pmemkv offers higher read performance and lower write amplification. But our intent is not to deter use of LSM, only to expand the choices developers and architects have for aligning workloads to backing stores.</description></item><item><title>What's coming in libpmemobj</title><link>https://pmem.io/blog/2017/01/whats-coming-in-libpmemobj/</link><pubDate>Wed, 25 Jan 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/01/whats-coming-in-libpmemobj/</guid><description>In my last post I&amp;rsquo;ve made a promise to share our plans for the near future. So here it is: 4 ideas that we are planning to ship with the upcoming version of libpmemobj.
Please note that most of our plans related to libpmemobj are available on our github issues page with the &amp;ldquo;Feature&amp;rdquo; label. Feel free to join the discussion!
Reserve/Initialize/Publish work-flow Many of persistent memory programming models proposed by researchers [1, 2] provide a different transactional semantics compared to libpmemobj.</description></item><item><title>Modeling strings with libpmemobj C++ bindings</title><link>https://pmem.io/blog/2017/01/modeling-strings-with-libpmemobj-c-bindings/</link><pubDate>Mon, 23 Jan 2017 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2017/01/modeling-strings-with-libpmemobj-c-bindings/</guid><description>C++ developers using libpmemobj have more than one option for modeling strings, depending on the size of the strings and whether they are fixed or varying in length. In this post we&amp;rsquo;ll review the representations that work, known variations to avoid, and finally present a persistent string class that implements these best practices.
Avoid wrapping fixed-size arrays You might expect (like I did at first!) that p&amp;lt;char[size]&amp;gt; is a proper way to simply model a fixed-size string, but actually this is not correct.</description></item><item><title>PMDK for Windows</title><link>https://pmem.io/blog/2016/12/pmdk-for-windows/</link><pubDate>Wed, 21 Dec 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/12/pmdk-for-windows/</guid><description>Throughout 2016 a team of engineers from Microsoft, Intel, HPE and HPI have been working to port the PMDK project to Windows and we are happy to announce that Technical Preview release is now available!
Our main goal in porting the library was to make it as easy as possible for developers to use the library in both Linux and Windows environments. To meet this goal, we focused on these elements as we undertook the effort:</description></item><item><title>libpmemobj - a year in review</title><link>https://pmem.io/blog/2016/12/libpmemobj-a-year-in-review/</link><pubDate>Tue, 20 Dec 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/12/libpmemobj-a-year-in-review/</guid><description>It&amp;rsquo;s been a while since the last post on our blog, but we&amp;rsquo;ve been busy with the recently released 1.2 version of the library. It comes packed with improvements all throughout the code base and it also brings a handful of new features that we hope will end up being useful.
With the year coming to an end, it&amp;rsquo;s a good time to look back and discuss the things we&amp;rsquo;ve learned and accomplished.</description></item><item><title>C++ bindings for libpmemobj (epilogue) - converting existing applications</title><link>https://pmem.io/blog/2016/06/c-bindings-for-libpmemobj-epilogue-converting-existing-applications/</link><pubDate>Thu, 02 Jun 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/06/c-bindings-for-libpmemobj-epilogue-converting-existing-applications/</guid><description>During the development of the C++ bindings, we wrote a couple of examples and even more tests. But these are new applications written from scratch to understand persistence. While this approach is OK for newly developed apps, there is a lot of existing code out there that is not designed for persistent memory. It would be a real shame, if the existing solutions couldn&amp;rsquo;t benefit from the existence of persistent memory because of the amount of work needed to redesign and change them.</description></item><item><title>C++ bindings for libpmemobj (part 7) - synchronization primitives</title><link>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-7-synchronization-primitives/</link><pubDate>Tue, 31 May 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-7-synchronization-primitives/</guid><description>To finish off the C++ bindings to libpmemobj blog marathon, I will introduce to you the synchronization mechanisms we implemented. They are mostly C++11-like implementations of different kinds of mutexes and the condition variable. They satisfy their respective concepts (Mutex, SharedMutex and so on), the difference is that they are based on the persistent memory resident synchronization primitives provided by libpmemobj.
Mutex The pmem::obj::mutex class satisfies the requirements of the Mutex and StandardLayoutType concepts.</description></item><item><title>C++ bindings for libpmemobj (part 6) - transactions</title><link>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-6-transactions/</link><pubDate>Wed, 25 May 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-6-transactions/</guid><description>As I mentioned in my previous blog post, transactions are the heart of libpmemobj. That is why we had to take utmost care while designing their C++ versions, so that they are as easy to use as possible. There are, however, a couple of compromises we had to make due to the inadequacies of the C++11 standard. That is why we encourage using the lambda, until the C++17 standard is more widely implemented.</description></item><item><title>C++ bindings for libpmemobj (part 5) - make_persistent</title><link>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-5-make_persistent/</link><pubDate>Thu, 19 May 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-5-make_persistent/</guid><description>One of the most important features of the C++ bindings to libpmemobj is the persistent_ptr smart pointer template. While using it is fairly straightforward, the allocation and object construction with the use of the C API is hard to get right. So like it&amp;rsquo;s C++ standard&amp;rsquo;s counterparts, it needed an allocation mechanism with appropriate object construction. This is exactly what this post will try to explain.
Transactional allocations Probably the most common usage of the allocating functions is within pmemobj transactions.</description></item><item><title>C++ bindings for libpmemobj (part 4) - pool handle wrapper</title><link>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-4-pool-handle-wrapper/</link><pubDate>Tue, 10 May 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/05/c-bindings-for-libpmemobj-part-4-pool-handle-wrapper/</guid><description>One of the necessary steps in developing the C++ libpmemobj bindings was the introduction of an abstraction of the C pool handle. We decided to do a very simple hierarchy where the pool template inherits from a generic pool_base. This was necessary to be able to have functions/methods which do not depend on the pool&amp;rsquo;s template argument. Please note that this makes both of these handles impossible to keep in persistent memory, due to the presence of a vtable.</description></item><item><title>Persistent allocator design - fragmentation</title><link>https://pmem.io/blog/2016/02/persistent-allocator-design-fragmentation/</link><pubDate>Thu, 25 Feb 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/02/persistent-allocator-design-fragmentation/</guid><description>Implementing a memory allocator is a balance between numerous properties with the two most important being time and space constraints. Making the malloc/free routines reasonably fast is a must for the implementation to be considered usable at all. The algorithm also mustn&amp;rsquo;t waste excessive amounts of memory.
During development of the library we quickly realized that the performance characteristics will be dominated by the number of cache line flushes that will be required to perform operations in a fail-safe atomic way.</description></item><item><title>How to emulate Persistent Memory</title><link>https://pmem.io/blog/2016/02/how-to-emulate-persistent-memory/</link><pubDate>Mon, 22 Feb 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/02/how-to-emulate-persistent-memory/</guid><description>Data allocated with PMDK is put to the virtual memory address space, and concrete ranges are relying on result of mmap(2) operation performed on the user defined files. Such files can exist on any storage media, however data consistency assurance embedded within PMDK requires frequent synchronisation of data that is being modified. Depending on platform capabilities, and underlying device where the files are, a different set of commands is used to facilitate synchronisation.</description></item><item><title>C++ bindings for libpmemobj (part 0)</title><link>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-0/</link><pubDate>Tue, 12 Jan 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-0/</guid><description>Our goal for the C pmemobj library was to make a fully featured implementation of persistent memory programming model without modifying the compiler. It&amp;rsquo;s meant for authors of very low-level system software and language creators. It&amp;rsquo;s not particularly pretty nor easy to use. The amount of macros, as well as the trickery inside them, might &amp;lsquo;amaze&amp;rsquo; even the biggest preprocessor fans ;)
The natural next step is to leverage the high-level languages features to create a more friendly, less error prone and generally nicer API.</description></item><item><title>C++ bindings for libpmemobj (part 1) - pmem resident variables</title><link>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-1-pmem-resident-variables/</link><pubDate>Tue, 12 Jan 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-1-pmem-resident-variables/</guid><description>One of the biggest hurdles and error prone things about our C API is that the user has to manually keep track of modifications to the persistent memory resident variables while in a transaction. A special semi-transparent template property class has been implemented to automatically add variable modifications to the transaction undo log.
pmem::obj::p Let&amp;rsquo;s start with the vector example from the previous tutorial series. It looked like this:
struct vector { int x; int y; int z; } PMEMoid root = pmemobj_root(pop, sizeof (struct vector)); struct vector *vectorp = pmemobj_direct(root); TX_BEGIN(pop) { pmemobj_tx_add_range(root, 0, sizeof (struct vector)); vectorp-&amp;gt;x = 5; vectorp-&amp;gt;y = 10; vectorp-&amp;gt;z = 15; } TX_END As you can see, the programmer has to remember to call pmemobj_tx_add_range function before any modifications to the memory.</description></item><item><title>C++ bindings for libpmemobj (part 2) - persistent smart pointer</title><link>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-2-persistent-smart-pointer/</link><pubDate>Tue, 12 Jan 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-2-persistent-smart-pointer/</guid><description>In our C API the programmer has to deal with custom pointers represented by the PMEMoid structure. Thanks to some macro magic we made it so that those PMEMoids are somewhat usable. C++ allows us to evolve this concept.
pmem::obj::persistent_ptr Almost everyone who ever touched a C++ code knows the idea behind smart pointers (for example, std::shared_ptr). Our persistent pointer works in the same way. It wraps around a type and provides implementation of operator*, operator-&amp;gt; and operator[].</description></item><item><title>C++ bindings for libpmemobj (part 3) - persistent queue example</title><link>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-3-persistent-queue-example/</link><pubDate>Tue, 12 Jan 2016 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2016/01/c-bindings-for-libpmemobj-part-3-persistent-queue-example/</guid><description>The best way to learn to code is usually by implementing an example. We are going to be creating a linked-list based queue data structure using the the pmem::obj::p and pmem::obj::persistent_ptr classes and libpmemobj C API. But first, a little bit of CS 101 :)
Linked-list queue Queue is a collection of elements with two important operations:
push - adds element to the tail of the structure pop - removes element from the head of the structure This makes the queue a First-In-First-Out (FIFO) data structure.</description></item><item><title>Performance improvements</title><link>https://pmem.io/blog/2015/12/performance-improvements/</link><pubDate>Tue, 15 Dec 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/12/performance-improvements/</guid><description>I would like to inform you about the performance improvements that have been going on in PMDK and libpmemobj in particular. We have not been standing still and we are trying out a couple of ideas on how to make our libraries even faster. Some of the improvements are smaller, some are larger. Some of them have already made it to the master branch and some are just ideas on how to rework the internals of libpmemobj to make it even faster.</description></item><item><title>An introduction to replication</title><link>https://pmem.io/blog/2015/11/an-introduction-to-replication/</link><pubDate>Mon, 23 Nov 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/11/an-introduction-to-replication/</guid><description>Replication is a means for raising the reliability of your pmemobj based applications. You can basically think of it as RAID 1 within PMDK. What happens is, when you write to your pool using the pmemobj_* (memcpy, persist, and so on) primitives, it gets copied to your replicas. Yes, you can have more than one replica. In fact you can have as many as you want, but you have to keep in mind the performance penalty.</description></item><item><title>Evaluation of a better object container</title><link>https://pmem.io/blog/2015/10/evaluation-of-a-better-object-container/</link><pubDate>Tue, 20 Oct 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/10/evaluation-of-a-better-object-container/</guid><description>During performance evaluation of our library, I asked myself a following question:
Which data structure has computational complexity of &amp;ldquo;insert at end&amp;rdquo; and &amp;ldquo;remove given element&amp;rdquo; operations no worse than a doubly-linked list, but with a smaller constant?
The point of that mental exercise was to come up with a persistent data structure that could replace doubly-linked list in object stores (right now a linked lists of every single user-allocated object) and undo logs.</description></item><item><title>pmemobjfs - The simple FUSE based on libpmemobj</title><link>https://pmem.io/blog/2015/09/pmemobjfs-the-simple-fuse-based-on-libpmemobj/</link><pubDate>Tue, 29 Sep 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/09/pmemobjfs-the-simple-fuse-based-on-libpmemobj/</guid><description>How to use it The sources of the pmemobjfs file system are available here. Please refer to README file for instructions on how to create a file system layout and mount it.
NOTE: This is just an example implementation of file system in user space using the libpmemobj library and it is not considered to be production quality. Please do not use this file system to store your data you care about because it may be lost.</description></item><item><title>Challenges of multi-threaded transactions</title><link>https://pmem.io/blog/2015/09/challenges-of-multi-threaded-transactions/</link><pubDate>Wed, 16 Sep 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/09/challenges-of-multi-threaded-transactions/</guid><description>Our library currently does not support threads cooperating (writing) within a single transaction. It does shift a lot of work from the library onto the user who now has to think about different parallelization solutions.
This was a conscious decision with iterative approach to creating the library in mind. It was far easier to implement the current transaction support strategy and say that it works with relatively good performance than it would be to implement multi-threaded transactions straight up and say that they work and scale linearly with CPUs thrown at them (yea right).</description></item><item><title>KV-store improved &amp; measured</title><link>https://pmem.io/blog/2015/09/kv-store-improved-measured/</link><pubDate>Thu, 10 Sep 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/09/kv-store-improved-measured/</guid><description>As promised in the previous post about the kv-store implementation I&amp;rsquo;m back with new results after implementing the optimizations I devised a month ago. As a bonus I implemented a red-black tree to have a fair comparison between two data structures that allocate similar number of nodes.
tl;dr: I was right about crit-bit :)
Test platform The same server was used to run the benchmarks but with the latest 4.2 kernel that contains numerous DAX improvements.</description></item><item><title>Transactional key-value store using libpmemobj - DIY</title><link>https://pmem.io/blog/2015/07/transactional-key-value-store-using-libpmemobj-diy/</link><pubDate>Fri, 31 Jul 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/07/transactional-key-value-store-using-libpmemobj-diy/</guid><description>Our library often gets compared to NoSQL databases because it stores things on storage in unstructured manner. Which is true, but, when you think about it, the pmemobj library is not technically a database, but can be used to implement one - like the MySQL storage engine example. In this post I&amp;rsquo;ll describe an example implementation of transactional kv-store, that has two different backends, which I&amp;rsquo;ll then compare. To make things more interesting, it&amp;rsquo;s not going to be your typical kv-store, since the data structure behind it won&amp;rsquo;t be a hashmap.</description></item><item><title>An introduction to pmemcheck (part 2) - transactions</title><link>https://pmem.io/blog/2015/07/an-introduction-to-pmemcheck-part-2-transactions/</link><pubDate>Mon, 20 Jul 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/07/an-introduction-to-pmemcheck-part-2-transactions/</guid><description>In my previous blog post I described the key features of the new persistent memory analysis tool we created - pmemcheck. You should now be aware of the main pitfalls of persistent memory programming and of ways pmemcheck informs you about possible misuses of PMEM. We should now dive into a more general approach of using persistent memory in a failsafe manner - transactions. This shouldn&amp;rsquo;t be an alien concept for anybody who had anything to do with databases.</description></item><item><title>An introduction to pmemcheck (part 1) - basics</title><link>https://pmem.io/blog/2015/07/an-introduction-to-pmemcheck-part-1-basics/</link><pubDate>Fri, 17 Jul 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/07/an-introduction-to-pmemcheck-part-1-basics/</guid><description>As you probably noticed from the previous posts, persistent memory programming isn&amp;rsquo;t really that easy. There are a couple of things you have to consider - data consistency being the most important one. The contemporary x86_64 architecture supports at most 8-byte atomic stores. You probably know by now, that by atomic I mean non-torn and not thread-safe. This means that you can be confident that you will not get 4 out of the 8 bytes with the new value and the rest will not be updated.</description></item><item><title>An introduction to pmemobj (part 7) - persistent lists</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-7-persistent-lists/</link><pubDate>Fri, 19 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-7-persistent-lists/</guid><description>The pmemobj library provides non-transactional persistent atomic circular doubly-linked lists (or NTPACDLL for short) API with an interface familiar to anyone who have ever included sys/queue.h header file - it&amp;rsquo;s in fact so similar that I considered not writing this post at all, you can just search the web for CIRCLEQ example.
Fun fact: The exact same list code is used internally by libpmemobj in the transaction undo log implementation.</description></item><item><title>An introduction to pmemobj (part 5) - atomic dynamic memory allocation</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-5-atomic-dynamic-memory-allocation/</link><pubDate>Thu, 18 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-5-atomic-dynamic-memory-allocation/</guid><description>In the previous post I talked about using transactions for allocating new objects, which is fine and is the most similar approach to the standard POSIX way. But it does add an overhead of maintaining an undo log of changes. A more optimal memory management can be achieved using the non-transactional atomic API the pmemobj library provides.
Fail-safe atomic allocations This API is not similar to the APIs most programmers are used to when it comes to handling memory.</description></item><item><title>An introduction to pmemobj (part 6) - threading</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-6-threading/</link><pubDate>Thu, 18 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-6-threading/</guid><description>All of the pmemobj library functions are thread-safe, with following two exceptions: pool management functions (open, close and friends) and pmemobj_root when providing different sizes in different threads - so as long as you are using this function the way it&amp;rsquo;s meant to be used you don&amp;rsquo;t have to worry about it. As for macros - generally only the FOREACH macros are not thread-safe for obvious reasons.
Synchronization If you need to put a lock inside a structure that resides on persistent memory, our library provides pthread-like API for that purpose.</description></item><item><title>An introduction to pmemobj (part 4) - transactional dynamic memory allocation</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-4-transactional-dynamic-memory-allocation/</link><pubDate>Wed, 17 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-4-transactional-dynamic-memory-allocation/</guid><description>This is a topic I intentionally avoided not to introduce too much complexity too fast. The pmemobj library contains an implemented from scratch memory allocator, that was designed with persistent memory in mind. There are two separate APIs: non-transactional and transactional.
Transactional allocations Let&amp;rsquo;s start with a simple snippet of volatile code:
struct rectangle { int a; int b; }; int area_calc(const struct rectangle *rect) { return rect-&amp;gt;a * rect-&amp;gt;b; } .</description></item><item><title>An introduction to pmemobj (part 3) - types</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-3-types/</link><pubDate>Tue, 16 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-3-types/</guid><description>In all of the previous post the code snippets and examples had persistent pointers (PMEMoid) without any type information - they were simple C structures. Very early in the development of the library we discovered that using something like that was extremely error-prone and generally difficult. That&amp;rsquo;s why considerable effort was put into encapsulating the PMEMoids with type-safe container. The end result can be compared with how shared_ptr and the like are done in C++11.</description></item><item><title>An introduction to pmemobj (part 2) - transactions</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-2-transactions/</link><pubDate>Mon, 15 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-2-transactions/</guid><description>By now you should be fairly familiar with the basics persistent memory programming, but to make sure the application is always in a consistent state you had to rely on your own solutions and tricks - like the length of a buffer in the previous example. Now, we will learn a generic solution provided by pmemobj to this type of problems - transactions. For now we will focus on a single-threaded applications with no locking.</description></item><item><title>An introduction to pmemobj (part 1) - accessing the persistent memory</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-1-accessing-the-persistent-memory/</link><pubDate>Sat, 13 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-1-accessing-the-persistent-memory/</guid><description>In the previous post, you learned a little bit about the general concept of the persistent memory programming model, now it&amp;rsquo;s time to start the coding ;)
Memory pools If you&amp;rsquo;ve read the overview you know that persistent memory is exposed by the OS as memory-mapped files, we call them pools.
The pmemobj library provides an interface to easily manage those pools, so that you don&amp;rsquo;t have to manually create the files or mmap them.</description></item><item><title>An introduction to pmemobj (part 0) - new programming model</title><link>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-0-new-programming-model/</link><pubDate>Fri, 12 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/an-introduction-to-pmemobj-part-0-new-programming-model/</guid><description>The aim of this tutorial series is to introduce you to programming with persistent, byte-addressable memory using the pmemobj library. We will go over all the available features, implement an example application and learn something about the inner workings of libpmemobj. If you haven&amp;rsquo;t read the NVM Library overview I encourage you to do that now.
When designing the library API, we&amp;rsquo;ve put a heavy emphasis on ease of use and &amp;ldquo;management explainability&amp;rdquo;, as well as flexibility and performance.</description></item><item><title>Type safety macros in libpmemobj</title><link>https://pmem.io/blog/2015/06/type-safety-macros-in-libpmemobj/</link><pubDate>Thu, 11 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/type-safety-macros-in-libpmemobj/</guid><description>The PMEMoid plays the role of a persistent pointer in a pmemobj pool. It consist of a shortened UUID of the pool which the object comes from and an offset relative to the beginning of the pool:
typedef struct pmemoid { uint64_t pool_uuid_lo; uint64_t off; } PMEMoid; Operating on such persistent pointers is equivalent to operating on raw pointers to volatile objects represented by void *. This approach is error prone and such errors are very hard to find.</description></item><item><title>Implementing (simple) MySQL storage engine with libpmemobj</title><link>https://pmem.io/blog/2015/06/implementing-simple-mysql-storage-engine-with-libpmemobj/</link><pubDate>Tue, 02 Jun 2015 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2015/06/implementing-simple-mysql-storage-engine-with-libpmemobj/</guid><description>The focus of the pmemobj library, like the name suggests, is storing objects on a persistent medium. A different, but very common, approach of doing exactly the same is to use a database with a specialized interface to manipulate the collection of data. MySQL is one such database, it processes SQL queries by calling (usually) multiple methods of the storage engine used for the data tables that query operates on. This tutorial covers basics of the pmemobj library, including non-transactional allocations and very simple transactions.</description></item><item><title>Using the Block Translation Table for sector atomicity</title><link>https://pmem.io/blog/2014/09/using-the-block-translation-table-for-sector-atomicity/</link><pubDate>Tue, 23 Sep 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/09/using-the-block-translation-table-for-sector-atomicity/</guid><description>Persistent memory based storage is able to perform IO at byte (or more accurately, cache line) granularity. However, we often want to expose such storage as traditional block devices. The block drivers for persistent memory will do exactly this. However, they do not provide any atomicity guarantees. Traditional SSDs typically provide protection against torn sectors in hardware, using stored energy in capacitors to complete in-flight block writes, or perhaps in firmware.</description></item><item><title>Git Workflow</title><link>https://pmem.io/blog/2014/09/git-workflow/</link><pubDate>Tue, 09 Sep 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/09/git-workflow/</guid><description>Now that we&amp;rsquo;ve created the GitHub Repository for the PMDK, here&amp;rsquo;s a more detailed description of the git workflow we&amp;rsquo;ve chosen. The basic idea is:
Current development happens on the master branch Releases are created periodically by tagging After a major release, a stable-1.x branch is created. All bug fixes should be committed to the oldest affected branch which is currently supported. Stable branches will be merged periodically to later branches and master.</description></item><item><title>NVM Library Overview</title><link>https://pmem.io/blog/2014/09/nvm-library-overview/</link><pubDate>Mon, 01 Sep 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/09/nvm-library-overview/</guid><description>[Edit on 2017-12-11: In the years since this entry was written, this work has evolved into PMDK.] Why are we building an NVM Library? Where does it live? How does it work? This blog entry provides some answers, which refer to this picture showing the overall library architecture:
Why? The operating system exposes persistent memory to applications as a memory-mapped file, using a persistent memory aware file system as shown in the picture.</description></item><item><title>Linux Examples</title><link>https://pmem.io/blog/2014/08/linux-examples/</link><pubDate>Fri, 29 Aug 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/08/linux-examples/</guid><description>[Edit on 2017-12-11: The Linux examples are outdated now, look at PMDK instead.] The basic architecture. for exposing persistent memory gives applications a very raw type of access. Applications can load/store directly to the persistence, but then what. What are the interesting problems facing an application developer and what would some solutions look like?
To help describe the issues and potential solutions, we&amp;rsquo;ve published a set of Linux examples around persistent memory.</description></item><item><title>Crawl, Walk, Run...</title><link>https://pmem.io/blog/2014/08/crawl-walk-run.../</link><pubDate>Wed, 27 Aug 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/08/crawl-walk-run.../</guid><description>If you can&amp;rsquo;t fly then run, if you can&amp;rsquo;t run then walk, if you can&amp;rsquo;t walk then crawl, but whatever you do you have to keep moving forward.
Martin Luther King Jr.
This project, as well as the support for persistent memory in various operating systems, can be thought of as a crawl, walk, run approach. As byte-addressable persistence enters the market, modified system BIOS images and device drivers expose it to other modules in the kernel.</description></item><item><title>References</title><link>https://pmem.io/blog/2014/08/references/</link><pubDate>Tue, 26 Aug 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/08/references/</guid><description>Some very interesting research has been happening in the area of Persistent Memory and more is emerging. While the examples provided here are meant as introductory and simple, some publications cover the topic in much more depth and include complete transaction systems, compiler/language enhancements, etc. Here are some of the most important publications in this space (please send links to more and we&amp;rsquo;ll include them).
One of the most impressive bodies of work in this area is Mnemosyne.</description></item><item><title>Creating pmem.io</title><link>https://pmem.io/blog/2014/08/creating-pmem.io/</link><pubDate>Mon, 25 Aug 2014 19:55:17 -0700</pubDate><guid>https://pmem.io/blog/2014/08/creating-pmem.io/</guid><description>The pmem project in GitHub has been created as an open source project focused on persistent memory programming. Everything on this web site and the associated GitHub repositories is open source under the &amp;ldquo;three-clause&amp;rdquo; BSD license.
Some educational Linux examples are included, which demonstrate the SNIA NVM programming model and some of the interesting features and challenges associated with persistent memory.
The team&amp;rsquo;s initial focus is the Linux NVM Library which will provide useful APIs for memory allocation, transactions, etc.</description></item></channel></rss>